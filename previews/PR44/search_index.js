var documenterSearchIndex = {"docs":
[{"location":"benchmarks/#Benchmark-test-functions","page":"Benchmark functions","title":"Benchmark test functions","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"The following benchmark functions are implemented, each function is defined in the survey[1]:","category":"page"},{"location":"benchmarks/#[Sphere](@ref)","page":"Benchmark functions","title":"Sphere","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"The Sphere function is defined as:","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"f(mathbfx) = sum_i=1^d x_i^2","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"with d the dimension of the design vector mathbfx, subject to 0 leq x_i leq 10.","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"The minimum is","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"f(mathbfx^*) = 0 quad mathbfx^* = (0 cdots 0)","category":"page"},{"location":"benchmarks/#[Easom](@ref)","page":"Benchmark functions","title":"Easom","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"The Easom function is defined as:","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"f(mathbfx) = -cos(x_1) cos(x_2) exp-(x_1 - pi)^2 - (x_2 - pi)^2","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"where the design vector is a 2-D vector only, subject to -100 leq x_i leq 100.","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"The function has the following minimum:","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"f(mathbfx^*) = -1 quad mathbfx^* = (pi pi)","category":"page"},{"location":"benchmarks/#[Ackley](@ref)","page":"Benchmark functions","title":"Ackley","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"The Ackley function is defined as:","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"f(mathbfx) = -20 expleft -002 sqrtfrac1dsum_i=1^dx_i^2 right\n- expleftfrac1dsum_i=1^dcos(2 pi x_i)right + 20 + e","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"where the design vector is a d-dimensional vector, subject to -35 leq x_i leq 35.","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"The function has the following minimum:","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"f(mathbfx^*) = 0 quad mathbfx^* = (0 cdots 0)","category":"page"},{"location":"benchmarks/#[Rosenbrock](@ref)","page":"Benchmark functions","title":"Rosenbrock","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"The famous Rosenbrock function is defined as:","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"f(mathbfx) = sum_i=1^N-1 left100(x_i-1-x_i^2)^2 +(1-x_i)^2 right","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"where the design vector is a N-dimensional vector, subject to -infty leq x_i leq infty.","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"The function has the following minimum:","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"f(mathbfx^*) = 0 quad mathbfx^* = (1 cdots 1)","category":"page"},{"location":"benchmarks/#[GoldsteinPrice](@ref)","page":"Benchmark functions","title":"GoldsteinPrice","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"The Goldstein-Price function is defined as:","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"f(xy)=1 + (x + y + 1)^2(19  14x+3x^2 14y + 6xy + 3y^2) times \n30 + (2x  3y)^2(18  32x + 12x^2 + 4y  36xy + 27y^2)","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"where x and y are the elements of a 2D design vector.","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"The function has the following minimum:","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"f(mathbfx^*) = 3 quad mathbfx^* = (0 -1)","category":"page"},{"location":"benchmarks/#[Beale](@ref)","page":"Benchmark functions","title":"Beale","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"The Beale function is defined as:","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"f(x y) = (15-x+xy)^2+(225-x+xy^2)^2+(2625-x+xy^3)^2","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"where x and y are the elements of a 2D design vector.","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"The function has the following minimum:","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"f(mathbfx^*) = 0 quad mathbfx^* = (3 05)","category":"page"},{"location":"benchmarks/#[Levy](@ref)","page":"Benchmark functions","title":"Levy","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"The Lévy function is defined as:","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"f(mathbfx) = sin^2pi w_1 + sum_i=1^d-1 (w_i-1)^2 1+10sin^2pi w_1 + 1\n+ (w_d-1)^2 1+sin^22pi w_d","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"where","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"w_i = 1 + fracx_i-14 and d is the dimension of the vector.","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"The function has the following minimum:","category":"page"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"f(mathbfx^*) = 0 quad mathbfx^* = (1 dots 1)","category":"page"},{"location":"benchmarks/#References","page":"Benchmark functions","title":"References","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmark functions","title":"Benchmark functions","text":"[1]: Jamil, M., & Yang, X. S. (2013). A literature survey of benchmark functions for global optimisation problems. International Journal of Mathematical Modelling and Numerical Optimisation, 4(2), 150–194. https://doi.org/10.1504/IJMMNO.2013.055204","category":"page"},{"location":"guide/#Reach","page":"Guide","title":"Reach","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"This package should/could be used by:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"Practitioners: people in need of a black box optimization framework for when they know very little about the problem at hand.\nStudents: wanting to learn about nature-inspired algorithms, stochastic optimization or want a general survey of the current literature.\nResearchers: who want to employ different algorithms at once, test them or use them as comparison for their own developed algorithms.","category":"page"},{"location":"guide/#A-primer-on-numerical-optimization","page":"Guide","title":"A primer on numerical optimization","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"Optimization is a huge subject, and I don't think Newton even realized this when discovering Calculus, where optimization has its roots. Basically, in optimization we are trying to find the best possible solution to a given problem. Worded in this way it seems that optimization is actually everywhere we look around, which is so very true, optimization is everywhere!","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"Say you like to run, and you look at your milage, timings and so on; you start to wonder, what is the best way to improve my timings? How can I maximize it?","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"Now imagine that you have some money to spare and you wish to invest it. What type of investment will return the largest profit and will also minimize the possible risk of losing money?","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"Optimization has been a major subject within analysis, the major branch of mathematics where most of its arguments come from. In mathematical language, we define an optimization problem as follows","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"textminimize f(mathbfx) quad mathbfx in mathbbR \ntextsubject to  h(mathbfx) = 0 \ntextand g(mathbfx) leq 0 ","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"h and g are referred to as constraint functions, and the full expressions with their equalities and inequalities are simply called constraints. When we have a problem like this, we call this a constrained optimization problem.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"On the other hand, if we only define the problem as","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"textminimize f(mathbfx)quad mathbfx in mathbbR","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"we are talking about an unconstrained optimization problem.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"The goal of optimization is to find the vector mathbfx that gives the lowest possible value for f given all the constraints, if any. The classic way to achieve this is by using derivatives and derivative tests, and throughout the years mathematicians have developed very rigorous and robust algorithms to find these values. Almost every procedure uses derivatives because Newton and Gauss taught us that these converge faster and more precisely to the true values. But recently, stochastic optimization algorithms, were randomness is used to guide the search for the best value, have been very popular and widely used within the scientific community.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"This is a very, very small space to talk about optimization, but the following references should get you started right away. [1], [2] and [3].","category":"page"},{"location":"guide/#On-Convergence","page":"Guide","title":"On Convergence","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"Convergence is a very strong word in mathematics, and it actually has lots of definitions depending on the specific branch of mathametics it is used. Here we shall use the numerical analysis definition, which is simply stated as a limit. We wish to obtain a value, whatever it is, in a finite time.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"We may employ tolerance values where we argue that a given solution is close to the real value that I know of. We can see this in the example above, where we know that the true value is a vector filled with zeros, but we don't actually obtain zeros, instead we get close values to zeros within a certain tolerance: in this scenario we can say that the optimization algorithm has converged.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"If, on the other hand, we rely on the number of maximum iterations then we can safely claim that when the algorithm has run for the number of maximum iterations then it has converged. Is that so? At least, in the realm of approximation algorithms we can safely claim that this is true.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"But don't take my word for it, in reality this is a very serious mathematical topic and should not be taken so slightly. Actually, every algorithm ever implemented must have a convergence analysis carried out for it, to ensure that either it will stop at some time or that it will given the desired result.","category":"page"},{"location":"guide/#The-basics-of-nature-and-bio-inspired-metaheuristics","page":"Guide","title":"The basics of nature and bio-inspired metaheuristics","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"Nature and bio-inspired metaheuristics work by means of two fundamental heuristics: exploration and exploitation.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"First, exploration is leveraged through the use of random numbers, these are created to try to cover most of the search space, i.e. the set of possible values that can be considered the solution to a given optimization problem. When exploring the search space, metaheuristics try to search as efficiently as possible, and most algorithms use uniform sampling to try and cover most, if not all, of the search space.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"Once the search space has been explored, the algorithm tries to identify, by means of some update rule, which of these proposed solutions are actually valid. In swarm intelligence algorithms such as Particle Swarm Optimization the different particles are ranked and checked against each other to see which has the most promising value. Then, exploitation kicks in, trying to take advantage of this information and trying to pull most of the swarm towards it.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"In the topic of optimization algorithms, nature and bio-inspired metaheuristics have a special place when talking about convergence, stability, and significance.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"First, convergence is usually measured as described in the section above, by means of a tolerance or a maximum number of iterations.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"Stability is a harder topic in this matter, because of the random aspect of most, if not all, of the current popular nature and bio-inspired metaheuristics. Reproduciblity is a big factor, and almost always algorithms need to be run independently at least 30 different times, with 30 statistically independent random number generators. But even this won't guarantee that every single run will give a good solution to the problem.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"At last, statistical significance is almost mandatory if one wants to have a solution that has an actual mathematical and statistical meaning. Because of randomness, the actual mechanism by which nature and bio-inspired metaheuristics are Markov Chains [4] which provide statistical tools to guarantee and promise that the values found are, indeed, the real ones. Hypothesis tests like the parametric t-test, the Mann-Whitney-Wilcoxon non-parametric test, and some others are the most popular statistical tests to prove significance of the values obtained from applying nature and bio-inspired metaheuristics.","category":"page"},{"location":"guide/#References","page":"Guide","title":"References","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"[4]: Yang, X.-S. (2014). Nature-inspired optimization algorithms. In Elsevier Insights. https://doi.org/10.1007/978-981-10-6689-4_8","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"[1]: https://en.wikipedia.org/wiki/Mathematical_optimization#History","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"[2]: https://web.stanford.edu/group/sisl/k12/optimization/MO-unit1-pdfs/1.1optimization.pdf","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"[3]: https://sites.math.northwestern.edu/~clark/publications/opti.pdf","category":"page"},{"location":"license/","page":"License","title":"License","text":"Newtman.jl was created by Edwin Bedolla and the package can be used with the following license:","category":"page"},{"location":"license/","page":"License","title":"License","text":"Copyright (c) 2019-2021 Edwin BedollaPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"EditURL = \"https://github.com/edwinb-ai/Newtman.jl/blob/master/docs/src/examples/examples.jl\"","category":"page"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Before we start, I will define a seed to enable reproducibility of the results presented here","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"RANDOM_SEED = 458012;\nnothing #hide","category":"page"},{"location":"examples/#Nonlinear-d-dimensional-global-optimization-problem","page":"Examples","title":"Nonlinear d-dimensional global optimization problem","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Using Newtman.jl is fairly straightforward, we will start by defining an d-dimensional nonlinear function to minimize, in this case we will use a popular function, the Griewank function defined as","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"f(mathbfx) = sum_i=1^d fracx_i^24000 - prod_i=1^d cosleft(\nfracx_isqrtiright) + 1","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"where d is the dimension of the problem. It's mostly evaluated within the boundaries -100 leq x_i leq 100, and it has a minimum at mathbf x^* = (0 cdots 0), and it evaluates to f(mathbfx^*) = 0.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"We define the function in Julia like this","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"function griewank(x::AbstractArray)\n    first_term = sum(x .^ 2) / 4000\n    # This variable will hold the result of the product,\n    # the second term in the function definition from above\n    second_term = 1.0\n    for (idx, val) in enumerate(x)\n        second_term *= cos(val / sqrt(idx))\n    end\n\n    return first_term - second_term + 1.0\nend","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Now, we wish to find the minimum of this function, and fortunately we know the true value so we can compare it later, we can use some of the implementations from Newtman.jl, for example, PSO. In this script we have chosen 30 particles within the population, d is equal to 20, next we define the boundaries and finally we declare that the algorithm will run for 20000 maximum iterations until it stops, having converged.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Newtman\n\nval = PSO(\n    griewank,\n    Population(35, 10, -600.0, 600.0; seed = RANDOM_SEED),\n    20000;\n    seed = RANDOM_SEED\n)\nprintln(val)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Within a certain tolerance of about epsilon = 1 times 10^-6 we have found the global minimum of the function. We can actually check the value with the evaluation, notice that it actually returns 0, as expected.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"griewank(val.x)","category":"page"},{"location":"examples/#Nonlinear-2-dimensional-global-optimization-problem","page":"Examples","title":"Nonlinear 2-dimensional global optimization problem","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Let us now tackle one of the most common optimization problems, which is finding the minimum of the Rosenbrock function which is a non-convex function, meaning that is does not have just one minimum or stationary point, it has several, so it is a difficult problem for classical optimization algorithms. In this example we will try to solve it using the SimulatedAnnealing implementation from Newtman.jl.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"First, we define the Rosenbrock function in Julia","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"rosenbrock2d(x) =  (1.0 - x[1]) ^ 2 + 100.0 * (x[2] - x[1] ^ 2) ^ 2;\nnothing #hide","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"We will apply the Simulated Annealing algorithm to find the global optimum","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"val = SimulatedAnnealing(\n    rosenbrock2d, -5.0, 5.0, 2; low_temp = 5000, seed = RANDOM_SEED\n)\nprintln(val)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Again, within a certain tolerance we find the expected result which is","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"mathbfx^* = (1 1)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"and if we account for rounding errors and floating-point arithmetic, we can safely take this result as the best.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Benchmarks","page":"Reference","title":"Benchmarks","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [Newtman.TestFunctions]\nPages = [\"TestFunctions.jl\",\n        \"implementations.jl\"]","category":"page"},{"location":"reference/#Newtman.TestFunctions.Benchmark","page":"Reference","title":"Newtman.TestFunctions.Benchmark","text":"Benchmark\n\nAbstract supertype for all benchmark functions.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Newtman.TestFunctions.Unconstrained","page":"Reference","title":"Newtman.TestFunctions.Unconstrained","text":"Unconstrained\n\nAbstract supertype for all unconstrained benchmark functions.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Newtman.TestFunctions.Ackley","page":"Reference","title":"Newtman.TestFunctions.Ackley","text":"Ackley\n\nAn unconstrained implementation of the d-dimensional Ackley function defined as:\n\nf(mathbfx) = -20 e^ -002 sqrtfrac1dsum_i=1^dx_i^2 - e^frac1dsum_i=1^dcos(2 pi x_i) + 20 + e\n\nwhere d is the dimension of the input vector mathbfx.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Newtman.TestFunctions.Beale","page":"Reference","title":"Newtman.TestFunctions.Beale","text":"Beale\n\nAn unconstrained implementation of the d-dimensional Beale function defined as:\n\nf(x y) = (15-x+xy)^2+(225-x+xy^2)^2+(2625-x+xy^3)^2\n\n\n\n\n\n","category":"type"},{"location":"reference/#Newtman.TestFunctions.Easom","page":"Reference","title":"Newtman.TestFunctions.Easom","text":"Easom\n\nAn unconstrained implementation of the 2-dimensional Easom function defined as:\n\nf(mathbfx) = -cos(x_1) cos(x_2) exp-(x_1 - pi)^2 - (x_2 - pi)^2\n\nwhere x_1 and x_2 refer to the first and second element of the input vector mathbfx.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Newtman.TestFunctions.GoldsteinPrice","page":"Reference","title":"Newtman.TestFunctions.GoldsteinPrice","text":"Goldstein-Price\n\nAn unconstrained implementation of the d-dimensional Goldstein-Price function defined as:\n\nf(xy)=1 + (x + y + 1)^2(19  14x+3x^2 14y + 6xy + 3y^2) times \n30 + (2x  3y)^2(18  32x + 12x^2 + 4y  36xy + 27y^2)\n\n\n\n\n\n","category":"type"},{"location":"reference/#Newtman.TestFunctions.Levy","page":"Reference","title":"Newtman.TestFunctions.Levy","text":"Lévy\n\nAn unconstrained implementation of the d-dimensional Lévy function defined as:\n\nf(mathbfx) = sin^2pi w_1 + sum_i=1^d-1 (w_i-1)^2 1+10sin^2pi w_1 + 1\n+ (w_d-1)^2 1+sin^22pi w_d\n\nwhere\n\nw_i = 1 + fracx_i-14 and d is the dimension of the vector.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Newtman.TestFunctions.Rosenbrock","page":"Reference","title":"Newtman.TestFunctions.Rosenbrock","text":"Rosenbrock\n\nAn unconstrained implementation of the d-dimensional Rosenbrock function defined as:\n\nf(mathbfx) = sum_i=1^N-1 left100(x_i-1-x_i^2)^2 +(1-x_i)^2 right\n\nwhere N is the dimension of the input vector mathbfx.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Newtman.TestFunctions.Sphere","page":"Reference","title":"Newtman.TestFunctions.Sphere","text":"Sphere\n\nAn unconstrained implementation of the Sphere function defined as:\n\nf(mathbfx) = sum_i=1^d x_i^2\n\nwhere d is the dimension of the input vector mathbfx.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Algorithms","page":"Reference","title":"Algorithms","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [Newtman]\nPages = [\"solvers/solvers.jl\", \"solvers/pso.jl\", \"solvers/simulated_annealing.jl\"]","category":"page"},{"location":"reference/#Newtman.Metaheuristic","page":"Reference","title":"Newtman.Metaheuristic","text":"Metaheuristic\n\nAbstract type for metaheuristic algorithms, this makes a clear distinction between different classifications of metaheuristic algorithms.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Newtman.OptimizationResults","page":"Reference","title":"Newtman.OptimizationResults","text":"OptimizationResults{T, U, V, W}\n\nType that formats the output of Solver to get better information from it.\n\nFields\n\nx::T: Stores the solution array from the solver, i.e. the solution that minimizes   the cost function.\nmin::U: Stores the value obtained from evaluating the cost function with   x, i.e. the minima found.\nimpl::AbstractString: Stores the name of the Solver used, i.e. the name or identifier of the   optimization algorithm.\niterations::Integer: Stores the number of maximum iterations that the solver was run.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Newtman.PopulationBase","page":"Reference","title":"Newtman.PopulationBase","text":"PopulationBase\n\nType for population-based algorithms that employ Population, i.e. subroutines that mutate an array of possible candidates in-place. An example of this type is PSO.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Newtman.Solver","page":"Reference","title":"Newtman.Solver","text":"Solver\n\nAbstract super-type for every algorithm implementation.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Newtman.PSO","page":"Reference","title":"Newtman.PSO","text":"PSO\n\nPSO is the type associated with the implementation for the Particle Swarm Optimization with momentum. See Algorithms for more information.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Newtman.PSO-Tuple{Function,AbstractArray,Int64}","page":"Reference","title":"Newtman.PSO","text":"PSO(f::Function, population::AbstractArray, k_max::Int;\n    w=0.9, c1=2.0, c2=2.0, seed = nothing\n    ) -> OptimizationResults\nPSO(f::Benchmark, population::AbstractArray, k_max::Int;\n    w=0.9, c1=2.0, c2=2.0, seed = nothing\n    ) -> OptimizationResults\n\nMethod that implements PSO for a function f of type Function or of type Benchmark. Returns an OptimizationResults type with information relevant to the run executed, see OptimizationResults.\n\nArguments\n\npopulation: can be any AbstractArray that contains Particle\n\ninstances, but it is expected to be generated by Population.\n\nk_max: number of maximum iterations until \"convergence\" of the algorithm.\n\nKeyword arguments\n\nIt is recommended to use the default values provided.\n\nw: value that controls how much of the initial velocity is retained, i.e.\n\nan inertia term. This values decays linearly over each iteration until it reaches the default miminum value of 0.4.\n\nc1: balance between the influence of the individual's knowledge, i.e. the\n\nbest inidividual solution so far.\n\nc2: balance between the influence of the population's knowledge, i.e. the\n\nbest global solution so far.\n\nseed: an integer to be used as the seed for the pseudo random number generators.\n\nIf nothing is passed (the default), then a random seed will be taken from the system.\n\nExamples\n\nusing Newtman\n\n# Define the Sphere function\nf_sphere(x) = sum(x .^ 2)\n\n# Implement PSO for a 3-dimensional Sphere function, with\n# 10000 iterations and 30 particles in the population.\nval = PSO(f_sphere, Population(30, 3, -15.0, 15.0), 10000)\n\n\n\n\n\n","category":"method"},{"location":"reference/#Newtman.GeneralSimulatedAnnealing-Union{Tuple{T}, Tuple{Function,T,T,Integer}} where T<:AbstractFloat","page":"Reference","title":"Newtman.GeneralSimulatedAnnealing","text":"GeneralSimulatedAnnealing(\n    f::Function, a::T, b::T, dim::Integer;\n    t0 = 500.0, low_temp = 5000, qv = 2.7, qa = -5.0,\n    seed = nothing\n    ) where {T <: AbstractFloat} -> OptimizationResults\nGeneralSimulatedAnnealing(\n    f::Benchmark, a::T, b::T, dim::Integer;\n    t0 = 500.0, low_temp = 20000, qv = 2.7, qa = -5.0\n    ) where {T <: AbstractFloat} -> OptimizationResults\n\nImplementation of the generalized version of simulated annealing. This implementation uses all the theory from Tsallis & Stariolo for the cooling schedule and the neighbor solution search. See GeneralSimulatedAnnealing for the implementation details.\n\nReturns an OptimizationResults type with information relevant to the run executed, see OptimizationResults.\n\nArguments\n\nf: any user defined Function that can take AbstractArray's.\na: lower bound for the solution search space.\nb: upper bound for the solution search space.\ndim: dimension of the optimization problem.\n\nKeyword arguments\n\nIt is recommended to use the default values provided.\n\nt0: initial value for the temperature that is used. The default is an okay\n\nvalue, but should be changed depending on the optimization problem.\n\nlow_temp: total number of iterations, short for lowering temperature steps.\n\nThis also corresponds to the famous Monte Carlo steps, which are the total number of steps until the algorithm finishes.\n\nqv: This is known as the Tsallis parameter; particularly this parameter\n\ncontrols the cooling schedule convergence and neighbor search. Positive values in the interval 153) are best because for values larger than 5/3 the neighbor search diverges.\n\nqa: Another Tsallis parameter; this particular parameter controls convergence\n\nfor the Metropolis-Hastings algorithm and the acceptance probability involved. The more negative the value is, the better, but Tsallis & Stariolo report that the default value is best.\n\nseed: an integer to be used as the seed for the pseudo random number generators.\n\nIf nothing is passed (the default), then a random seed will be taken from the system.\n\nExamples\n\nusing Newtman\n\n# Define the 2D Rosenbrock function\nrosenbrock2d(x) =  (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n\n# Implement Simulated Annealing for a 2-dimensional Rosenbrock function, with\n# 15000 iterations and default values.\nval = GeneralSimulatedAnnealing(rosenbrock2d, -5.0, 5.0, 2; low_temp = 15000)\n\n\n\n\n\n","category":"method"},{"location":"reference/#Newtman.SimulatedAnnealing-Union{Tuple{T}, Tuple{Function,T,T,Integer}} where T<:AbstractFloat","page":"Reference","title":"Newtman.SimulatedAnnealing","text":"SimulatedAnnealing(f::Function, a::T, b::T, dim::Integer;\n    t0 = 500.0, low_temp = 5000, seed = nothing\n    ) where {T <: AbstractFloat} -> OptimizationResults\nSimulatedAnnealing(f::Benchmark, a::T, b::T, dim::Integer;\n    t0 = 500.0, low_temp = 5000) where {T <: AbstractFloat} -> OptimizationResults\n\nImplementation of the classical version of simulated annealing. This implementation uses a logarithmic cooling schedule and searches possible candidate solutions by sampling from an approximate Boltzmann distribution, drawn as a normal distribution.\n\nReturns an OptimizationResults type with information relevant to the run executed, see OptimizationResults.\n\nArguments\n\nf: any user defined Function that can take AbstractArray's.\na: lower bound for the solution search space.\nb: upper bound for the solution search space.\ndim: dimension of the optimization problem.\n\nKeyword arguments\n\nIt is recommended to use the default values provided.\n\nt0: initial value for the temperature that is used. The default is an okay\n\nvalue, but should be changed depending on the optimization problem.\n\nlow_temp: total number of iterations, short for lowering temperature steps.\n\nThis also corresponds to the famous Monte Carlo steps, which are the total number of steps until the algorithm finishes.\n\nseed: an integer to be used as the seed for the pseudo random number generators.\n\nIf nothing is passed (the default), then a random seed will be taken from the system.\n\nExamples\n\nusing Newtman\n\n# Define the 2D Rosenbrock function\nrosenbrock2d(x) =  (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n\n# Implement Simulated Annealing for a 2-dimensional Rosenbrock function, with\n# 5000 iterations.\nval = SimulatedAnnealing(rosenbrock2d, -5.0, 5.0, 2; low_temp = 5000)\n\n\n\n\n\n","category":"method"},{"location":"reference/#Population","page":"Reference","title":"Population","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [Newtman]\nPages = [\"population/types.jl\"]","category":"page"},{"location":"reference/#Newtman.Individual","page":"Reference","title":"Newtman.Individual","text":"Individual\n\nAbstract super-type for types that contain their own information.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Newtman.Particle-Union{Tuple{V}, Tuple{T}, Tuple{T,T,T,V,V}} where V<:AbstractFloat where T<:AbstractArray","page":"Reference","title":"Newtman.Particle","text":"Particle(x::T, v::T, x_best::T, a::V, b::V)\n    where {T<:AbstractArray, V<:AbstractFloat}\n\nA type that can hold information about current position, current velocity, the best candidate to a solution, as well as defining the bounds. The dimensions of the Particle are inferred from the length of the arrays.\n\nArguments\n\nx: Array that holds the positions of possible solutions.\nv: Array that holds velocities related to x.\nx_best: An element of x that determines the best position for the particle.\na: lower bound for x\nb: upper bound for v\n\nExample\n\np = Particle(zeros(3), rand(3), zeros(3), -1.0, 1.0)\n\n\n\n\n\n","category":"method"},{"location":"reference/#Newtman.Particle-Union{Tuple{V}, Tuple{T}, Tuple{T,T,V}} where V<:Int64 where T<:AbstractFloat","page":"Reference","title":"Newtman.Particle","text":"Particle(a::T, b::T, n::V)\n    where {T<:AbstractFloat, V<:Int}\n\nParticle that can be created randomly using the bounds and the dimension needed.\n\nArguments\n\na: lower bound for x\nb: upper bound for v\nn: dimension for x, v, and x_best.\n\nExample\n\np = Particle(-1.0, 1.0, 3)\n\n\n\n\n\n","category":"method"},{"location":"reference/#Newtman.Population-Tuple{Integer,Integer,Vararg{Any,N} where N}","page":"Reference","title":"Newtman.Population","text":"Population(num_particles::Integer, dim::Integer, x...)\n    -> Vector{Particle}(undef, num_particles)\n\nAn array of Particle's where each of them are bounded and are given a dimension. x is a tuple of ranges for each dimension for the Particle's specified.\n\nArguments\n\nnum_particles: Number of particles in the Population.\ndim: Dimension for every Particle.\nx: Tuple of ranges for each dimension.\n\nExample\n\n# Two ranges, one for each dimension\nrange_a = SVector(-10.0, 10.0)\nrange_b = SVector(-2.5, 2.0)\npops = Population(2, 20, ranges_a, range_b)\n\n\n\n\n\n","category":"method"},{"location":"reference/#Newtman.Population-Union{Tuple{V}, Tuple{T}, Tuple{T,T,V,V}} where V<:AbstractFloat where T<:Int64","page":"Reference","title":"Newtman.Population","text":"Population(num_particles::T, dim::T, a::V, b::V)\n    where {T<:Int, V<:AbstractFloat} -> Vector{Particle}(undef, num_particles)\n\nAn array of Particles where each of them are bounded and are given a dimension. This is essentially a multi-dimensional array. It makes handling Particles much easier.\n\nArguments\n\nnum_particles: Number of particles in the Population.\ndim: Dimension for every Particle.\na: Lower bound for every Particle, this is shared across every instance.\nb: Upper bound for every Particle, this is shared across every instance.\n\nExample\n\npop = Population(35, 4, -1.0, 1.0)\n\n\n\n\n\n","category":"method"},{"location":"#Newtman.jl","page":"Home","title":"Newtman.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is Newtman.jl, an stochastic optimization package that implements a number of metaheuristic algorithms, mostly nature-inspired and bio-inspired.","category":"page"},{"location":"#On-Metaheuristics","page":"Home","title":"On Metaheuristics","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The term metaheuristic has some history behind it, and a large span of definitions within the scientific community. In Newtman.jl, metaheuristic is defined as follows","category":"page"},{"location":"","page":"Home","title":"Home","text":"A metaheuristic is a black box optimization framework that employs heuristics to find a close-to optimal solution for a given optimization problem.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The definition is important in this context. Newtman.jl strives on implementations of already existent algorithms, but actually the term itself has not yet been acquired a formal definition, as Sörensen[1] says in his paper.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Because Newtman.jl employs metaheuristics as actual black box optimization frameworks, this definition should be enough to provide the actual purpose of the package.","category":"page"},{"location":"#Nature-and-bio-inspired-algorithms","page":"Home","title":"Nature and bio-inspired algorithms","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Heuristic is a term for a simple rule. Given a rule, mainly provided by experience, one can create a process or algorithm to solve a given problem.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Nature is an unlimited source of experience that can provide a lot of heuristics for us, if one looks closely.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Recent research has taken this approach[2] and scientists have taken inspiration from nature to create heuristics for problem solving. Examples are Ant Colony Optimization[3], based on the foraging behavior of ant colonies, Particle Swarm Optimization[4], which draws inspiration in the so-called swarm intelligence of birds, people, and so on.","category":"page"},{"location":"","page":"Home","title":"Home","text":"With this mindset, the field of stochastic optimization witnessed an avalanche of \"novel\" algorithms all based on nature, which were called nature-inspired metaheuristics.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If one takes not only nature, but physics, chemistry and biology, one can create the so-called bio-inspired metaheuristics, based on ideas drawn from biological, chemical and physical processes.","category":"page"},{"location":"#The-need-for-Newtman.jl-and-nature-inspired-algorithms","page":"Home","title":"The need for Newtman.jl and nature-inspired algorithms","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Most metaheuristic algorithms are not very difficult to implement, and most of the time there is already a package for it (e.g. NiaPy for Python).","category":"page"},{"location":"","page":"Home","title":"Home","text":"In general, these algorithms are not consistently used because there are more robust and exact algorithms out there, such as the classics BFGS, L-BFGS, Gradient Descent and many more.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Nonetheless, there are several areas in optimization that need quick and approximate solutions, mostly because they are unsolvable in finite time[1]. Because of this, nature and bio-inspired algorithms rose to the top in some of these problems and were the only framework that could give a reasonable solution.","category":"page"},{"location":"","page":"Home","title":"Home","text":"When there is not much information about the problem (i.e. the derivative or gradient of the fitness function), or if the classic algorithms take too much time to converge, nature and bio-inspired algorithms tackle these types of problems with randomness and heuristics, and having found a close-to optimal solution, more robust algorithms can be applied to the given problem. [5]","category":"page"},{"location":"#About-using-Julia","page":"Home","title":"About using Julia","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In scientific computing, the Julia programming language has been an excellent tool to get rid of the two language problem, i.e. when there is a need for high perfomance calculations but with low level manipulation, problems can be harder to code and even harder to solve. Julia is a great candidate to solve this problem, and this package attempts to prove that by providing high perfomant code and implementations.","category":"page"},{"location":"#The-Evolutionary-Computation-Bestiary","page":"Home","title":"The Evolutionary Computation Bestiary","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"There is a hidden goal for Newtman.jl and that is to implement all of the algorithms proposed in the EC Bestiary[6], which is a compilation of most of the nature and bio-inspired algorithms in the literature.","category":"page"},{"location":"#On-the-name-of-the-package","page":"Home","title":"On the name of the package","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Because of this hidden goal, the name Newtman was chosen, which is a pormanteau of the character by J.K. Rowling, Newton Scamander, whose purpose in life is to collect samples of fantastic beasts and create a book or log of their nature.","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"[1]: Sörensen, K. (2015). Metaheuristics-the metaphor exposed. International Transactions in Operational Research, 22(1), 3–18. https://doi.org/10.1111/itor.12001","category":"page"},{"location":"","page":"Home","title":"Home","text":"[2]: Kar, A. K. (2016). Bio inspired computing - A review of algorithms and scope of applications. Expert Systems with Applications, 59, 20–32. https://doi.org/10.1016/j.eswa.2016.04.018","category":"page"},{"location":"","page":"Home","title":"Home","text":"[3]: Dorigo, M., & Di Caro, G. (1999, July). Ant colony optimization: a new meta-heuristic. In Proceedings of the 1999 congress on evolutionary computation-CEC99 (Cat. No. 99TH8406) (Vol. 2, pp. 1470-1477). IEEE.","category":"page"},{"location":"","page":"Home","title":"Home","text":"[4]: Eberhart, R., & Kennedy, J. (1995, November). Particle swarm optimization. In Proceedings of the IEEE international conference on neural networks (Vol. 4, pp. 1942-1948).","category":"page"},{"location":"","page":"Home","title":"Home","text":"[5]: Luke, S. (2011). Essentials of metaheuristics. In Genetic Programming and Evolvable Machines (Vol. 12). https://doi.org/10.1007/s10710-011-9139-0","category":"page"},{"location":"","page":"Home","title":"Home","text":"[6]: Felipe Campelo, & Claus Aranha. (2018, June 20). EC Bestiary: A bestiary of evolutionary, swarm and other metaphor-based algorithms (Version v2.0.1). Zenodo. http://doi.org/10.5281/zenodo.1293352","category":"page"},{"location":"algorithms/#implementations-docs","page":"Implementations","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"The following algorithms are implemented:","category":"page"},{"location":"algorithms/#Particle-Swarm-Optimization","page":"Implementations","title":"Particle Swarm Optimization","text":"","category":"section"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"This implementation is the modified Particle Swarm Optimization [1] where it employs an inertia weight omega that controls convergence. This implementation uses linear decay for the inertia weight, which lowers the value of  omega iteratively until it reaches the default minimum of omega = 04.","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"The update rules for the particles are the following:","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"x_i+1 = x_i + v_i+1 \nv_i+1 = omega v_i + varphi_1 beta_1 (p_i - x_i) + varphi_2 beta_2 (p_g - x_i) \nomega = omega - eta","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"where beta_1 and beta_2 are uniformly distributed random numbers; varphi_1 and varphi_2 are the momentum coefficients; p_i is the previous individual best position and p_g is the privious global best position of the population; finally eta is the weight decay, currently implemented as","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"eta = frac(09 - 04)n","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"where 09 is the original default value for omega, 04 is the default minimum as explained before and n is the total number of iterations the algorithm is run. This guarantees that the weight decays linearly.","category":"page"},{"location":"algorithms/#Simulated-Annealing","page":"Implementations","title":"Simulated Annealing","text":"","category":"section"},{"location":"algorithms/#Classic-version","page":"Implementations","title":"Classic version","text":"","category":"section"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"This implementation [2] uses the following logarithmic cooling schedule","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"T_new(t) = T_0 fraclog(2)log1 + t","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"to obtain a new temperature each iteration, starting from an initial temperature T_0.","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"This implementation searches possible candidate solutions by sampling from an approximate Boltzmann distribution, drawn as a normal distribution like so","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"g_sol(x) = mathcalN(0 1) * sigma \nx_sol = x_prev + g_sol(x)","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"where g_sol(x) is an array filled with random values sampled from an approximate normal standard distribution with standard deviation sigma = sqrtT_new(t) which corresponds to the previously found temperature. With this array, the new solution is computed as the previous (best) solution, x_sol, and adding the sampled array.","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"Finally, the Metropolis-Hastings algorithm is defined as","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"P(x_sol leftarrow x_old) = 10 qquad f(x_sol)  f(x_old) \nP(x_sol leftarrow x_old) = e^(Delta  T_new(t)) qquad f(x_sol) geq f(x_old)","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"where Delta = f(x_old) - f(x_sol).","category":"page"},{"location":"algorithms/#Generalized-version","page":"Implementations","title":"Generalized version","text":"","category":"section"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"This is the implementation developed by Tsallis & Stariolo [2] following Tsallis' theory on the generalization of the extensive entropy developed in Statistical Mechanics. This implementation has as special cases the classic version described above and the fast version developed by Szu & Hartley [3].","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"This implementation is quite interesting as it uses the Tsallis entropy defined [4] as","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"S_q = k_B frac1-sum_q p_i^qq_1","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"where q is the so-called Tsallis parameter. By developing fundamental Statistical Mechanics theory and combining it with Gibbs' ensemble theory (I'm skipping all the development, it's quite extensive, pun intended), we arrive at a Metropolis-Hastings-like acceptance probability","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"P_q_a(x_sol leftarrow x_old) = 1 qquad f(x_sol)  f(x_old) \n\nP_q_a(x_sol leftarrow x_old) = frac11+(q_a-1)Delta  T_new(t)^(1(q_a-1)) qquad f(x_sol) geq f(x_old)","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"there is a lot to unpack here, so we will go step by step.","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"Firstly, the cooling schedule is now defined as","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"T_q_v^V (t) = frac2^q_v-1-1(1+t)^q_v-1-1","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"T_new(t) = T_0 T_q_v^V (t)","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"where T_0 is the initial temperature.","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"Then we have the neighbor sampling distribution defined as","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"g_q_v(Delta x) = left(fracq_v-1piright)^D2\n\nfracGammaleft(frac1q_v-1+fracD-12right)Gammaleft(frac1q_v-1-frac12right)\n\nfracT_q_v^V (t)^-fracD3-q_vleft(1+(q_v-1)frac(Delta x)^2T_q_v^V (t)^frac23-q_vright)^frac1q_V+1+fracD-12 \n\nforall D forall q_V","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"where Delta x = x_sol - x_old, and Gamma denotes the Gamma function. As you can see, this distribution is very complicated, but in the original paper [2] Tsallis & Stariolo provide an algorithm where they approximate this distribution as a Lévy distribution. This algorithm is provided in the paper and it's the one used in this implementation.","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"With this, we update the neighbor solution as before x_sol = x_old + g_q_v(Delta x) and then we employ the Metropolis-Hastings algorithm.","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"Lastly, an important case is when q_A  0 because the update rule is now undetermined, so we employ the following update rule","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"P_q_a(x_sol leftarrow x_old) = 1 qquad f(x_sol)  f(x_old) \n\nP_q_a(x_sol leftarrow x_old) = 1+(q_a-1)Delta  T_new(t)^-1 qquad f(x_sol) geq f(x_old)","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"which means that we always reject the solution x_sol whenever the proposition 1+(q_a-1)Delta  T_new(t)  0 is true.","category":"page"},{"location":"algorithms/#References","page":"Implementations","title":"References","text":"","category":"section"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"[1]: Eberhart, R. C., & Shi, Y. (2000, July). Comparing inertia weights and constriction factors in particle swarm optimization. In Proceedings of the 2000 congress on evolutionary computation. CEC00 (Cat. No. 00TH8512) (Vol. 1, pp. 84-88). IEEE.","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"[2]: C. Tsallis and D. A. Stariolo, “Generalized Simulated Annealing,” Physica A: Statistical Mechanics and its Applications, vol. 233, no. 1–2, pp. 395–406, Nov. 1996, doi: 10.1016/S0378-4371(96)00271-3.","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"[3]: Szu, H., & Hartley, R. (1987). Fast simulated annealing. Physics letters A, 122(3-4), 157-162.","category":"page"},{"location":"algorithms/","page":"Implementations","title":"Implementations","text":"[4]: C. Tsallis, “Possible generalization of Boltzmann-Gibbs statistics,” J Stat Phys, vol. 52, no. 1, pp. 479–487, Jul. 1988, doi: 10.1007/BF01016429.","category":"page"}]
}
