<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Guide · Newtman.jl</title><link rel="canonical" href="https://edwinb-ai.github.io/Newtman.jl/stable/guide/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Newtman.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Guide</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#A-primer-on-numerical-optimization"><span>A primer on numerical optimization</span></a></li><li class="toplevel"><a class="tocitem" href="#On-Convergence"><span>On Convergence</span></a></li><li class="toplevel"><a class="tocitem" href="#The-basics-of-nature-and-bio-inspired-metaheuristics"><span>The basics of nature and bio-inspired metaheuristics</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../examples/">Examples</a></li><li><a class="tocitem" href="../algorithms/">Implementations</a></li><li><a class="tocitem" href="../benchmarks/">Benchmark functions</a></li><li><a class="tocitem" href="../reference/">Reference</a></li><li><a class="tocitem" href="../license/">License</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Guide</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Guide</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/edwinb-ai/Newtman.jl/blob/master/docs/src/guide.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Reach"><a class="docs-heading-anchor" href="#Reach">Reach</a><a id="Reach-1"></a><a class="docs-heading-anchor-permalink" href="#Reach" title="Permalink"></a></h1><p>This package should/could be used by:</p><ul><li><strong>Practitioners</strong>: people in need of a <em>black box optimization</em> framework for when they know very little about the problem at hand.</li><li><strong>Students</strong>: wanting to learn about nature-inspired algorithms, stochastic optimization or want a general survey of the current literature.</li><li><strong>Researchers</strong>: who want to employ different algorithms at once, test them or use them as comparison for their own developed algorithms.</li></ul><h1 id="A-primer-on-numerical-optimization"><a class="docs-heading-anchor" href="#A-primer-on-numerical-optimization">A primer on numerical optimization</a><a id="A-primer-on-numerical-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#A-primer-on-numerical-optimization" title="Permalink"></a></h1><p><strong>Optimization</strong> is a huge subject, and I don&#39;t think Newton even realized this when discovering Calculus, where optimization has its roots. Basically, in optimization we are trying to find <strong>the best</strong> possible solution to a given problem. Worded in this way it seems that optimization is actually everywhere we look around, which is so very true, optimization is everywhere!</p><p>Say you like to run, and you look at your milage, timings and so on; you start to wonder, what <em>is the best</em> way to <strong>improve</strong> my timings? How can I <strong>maximize</strong> it?</p><p>Now imagine that you have some money to spare and you wish to invest it. What type of investment will return the <strong>largest</strong> profit and will also <strong>minimize</strong> the possible risk of losing money?</p><p><em>Optimization</em> has been a major subject within <em>analysis</em>, the major branch of mathematics where most of its arguments come from. In mathematical language, we define an <strong>optimization problem</strong> as follows</p><div>\[\text{minimize} f(\mathbf{x}), \quad \mathbf{x} \in \mathbb{R} \\
\text{subject to } h(\mathbf{x}) = 0, \\
\text{and }g(\mathbf{x}) \leq 0 .\]</div><p><code>h</code> and <code>g</code> are referred to as <strong>constraint functions</strong>, and the full expressions with their equalities and inequalities are simply called <strong>constraints</strong>. When we have a problem like this, we call this a <strong>constrained optimization problem</strong>.</p><p>On the other hand, if we only define the problem as</p><div>\[\text{minimize} f(\mathbf{x}),\quad \mathbf{x} \in \mathbb{R}\]</div><p>we are talking about an <strong>unconstrained optimization problem</strong>.</p><p>The goal of optimization is to find the <em>vector</em> <span>$\mathbf{x}$</span> that gives the <strong>lowest</strong> possible value for <code>f</code> given all the constraints, if any. The classic way to achieve this is by using <em>derivatives</em> and <em>derivative tests</em>, and throughout the years mathematicians have developed very rigorous and robust algorithms to find these values. Almost every procedure uses <em>derivatives</em> because Newton and Gauss taught us that these <em>converge</em> faster and more precisely to the true values. But recently, <em>stochastic optimization</em> algorithms, were randomness is used to guide the search for the best value, have been very popular and widely used within the scientific community.</p><p>This is a very, very small space to talk about optimization, but the following references should get you started right away. <sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>, <sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup> and <sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup>.</p><h1 id="On-Convergence"><a class="docs-heading-anchor" href="#On-Convergence">On Convergence</a><a id="On-Convergence-1"></a><a class="docs-heading-anchor-permalink" href="#On-Convergence" title="Permalink"></a></h1><p><strong>Convergence</strong> is a very strong word in mathematics, and it actually has lots of definitions depending on the specific branch of mathametics it is used. Here we shall use the <em>numerical analysis</em> definition, which is simply stated as a limit. We wish to obtain a value, whatever it is, in a finite time.</p><p>We may employ <em>tolerance</em> values where we argue that a given solution is <strong>close to</strong> the real value that I know of. We can see this in the example above, where we know that the true value is a <em>vector</em> filled with zeros, but we don&#39;t actually obtain zeros, instead we get <em>close</em> values to zeros within a certain <em>tolerance</em>: in this scenario we can say that the optimization algorithm <strong>has converged</strong>.</p><p>If, on the other hand, we rely on the number of <strong>maximum iterations</strong> then we can safely claim that when the algorithm has run for the number of <em>maximum iterations</em> then it has converged. Is that so? At least, in the realm of <a href="https://en.wikipedia.org/wiki/Approximation_algorithm">approximation algorithms</a> we can safely claim that this is true.</p><p>But don&#39;t take my word for it, in reality this is a very serious mathematical topic and should not be taken so slightly. Actually, every algorithm ever implemented must have a <strong>convergence analysis</strong> carried out for it, to ensure that either it will stop at some time or that it will given the desired result.</p><h1 id="The-basics-of-nature-and-bio-inspired-metaheuristics"><a class="docs-heading-anchor" href="#The-basics-of-nature-and-bio-inspired-metaheuristics">The basics of nature and bio-inspired metaheuristics</a><a id="The-basics-of-nature-and-bio-inspired-metaheuristics-1"></a><a class="docs-heading-anchor-permalink" href="#The-basics-of-nature-and-bio-inspired-metaheuristics" title="Permalink"></a></h1><p><strong>Nature and bio-inspired metaheuristics</strong> work by means of two fundamental <em>heuristics</em>: <strong>exploration</strong> and <strong>exploitation</strong>.</p><p>First, <strong>exploration</strong> is leveraged through the use of <em>random numbers</em>, these are created to try to cover most of the <em>search space</em>, i.e. the set of possible values that can be considered the solution to a given optimization problem. When <em>exploring</em> the <em>search space</em>, metaheuristics try to search as efficiently as possible, and most algorithms use <em>uniform sampling</em> to try and cover most, if not all, of the search space.</p><p>Once the <em>search space</em> has been explored, the algorithm tries to identify, by means of some update rule, which of these proposed solutions are actually valid. In <em>swarm intelligence</em> algorithms such as <a href="../algorithms/#implementations-docs"><code>Particle Swarm Optimization</code></a> the different particles are ranked and checked against each other to see which has the most promising value. Then, <strong>exploitation</strong> kicks in, trying to take advantage of this information and trying to pull most of the swarm towards it.</p><p>In the topic of optimization algorithms, <em>nature and bio-inspired metaheuristics</em> have a special place when talking about <em>convergence</em>, <em>stability</em>, and <em>significance.</em></p><p>First, <em>convergence</em> is usually measured as described in the section above, by means of a <em>tolerance</em> or a <em>maximum number</em> of iterations.</p><p><em>Stability</em> is a harder topic in this matter, because of the random aspect of most, if not all, of the current popular <em>nature and bio-inspired metaheuristics.</em> Reproduciblity is a big factor, and almost always algorithms need to be run independently <em>at least</em> 30 different times, with 30 statistically independent random number generators. But even this won&#39;t guarantee that every single run will give a good solution to the problem.</p><p>At last, <em>statistical significance</em> is almost mandatory if one wants to have a solution that has an actual mathematical and statistical <em>meaning.</em> Because of randomness, the actual mechanism by which <em>nature and bio-inspired metaheuristics</em> are Markov Chains <sup class="footnote-reference"><a id="citeref-4" href="#footnote-4">[4]</a></sup> which provide statistical tools to guarantee and promise that the values found are, indeed, the real ones. <em>Hypothesis tests</em> like the parametric <em>t-test</em>, the <em>Mann-Whitney-Wilcoxon</em> non-parametric test, and some others are the most popular statistical tests to prove <em>significance</em> of the values obtained from applying <em>nature and bio-inspired metaheuristics.</em></p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-4"><a class="tag is-link" href="#citeref-4">4</a>Yang, X.-S. (2014). Nature-inspired optimization algorithms. In Elsevier Insights. <a href="https://doi.org/10.1007/978-981-10-6689-4_8">https://doi.org/10.1007/978-981-10-6689-4_8</a></li><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>https://en.wikipedia.org/wiki/Mathematical_optimization#History</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>https://web.stanford.edu/group/sisl/k12/optimization/MO-unit1-pdfs/1.1optimization.pdf</li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a>https://sites.math.northwestern.edu/~clark/publications/opti.pdf</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../examples/">Examples »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 13 January 2021 21:23">Wednesday 13 January 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
